{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open questions \n",
    "\n",
    "06.05.21 - K Q: Do we need if __name__ == \"__main__\" for multiprocessing? \n",
    "Open Q: Will commas be removed by the tagger?\n",
    "Should we transform all 'word_tuple's into 'tagged_sentence[index]'? Or does it improve readability?\n",
    "\n",
    "New comments\n",
    "10.06.21: Flair tagger seems to be much slower (>30min for 3 files for me). Should we separate tagging and feature collection? \n",
    "For example, we could tag the file, save it in the folder separately, and then use the tagged file for feature collection later\n",
    "Then if we have to change or re-run, we would have a copy of tagging / could do it separately to help with time / CPU power \n",
    "Would take more hard drive space, but could also have it be optional, i.e. if tagged version exists, use that, if not, tag fresh and save\n",
    "I've added a multiprocessing system for which the feature dicts for each file get saved separately\n",
    "These can easily be combined into one file later and would allow us to divided up parts of Reddit across multiple computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import flair\n",
    "import re\n",
    "import time\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool, Manager\n",
    "import psutil\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "# import advertools as adv\n",
    "from datetime import timedelta\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the FLAIR tagger (if not using the NLTK tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_FLAIR = SequenceTagger.load(\"C:/Users/gusta/Documents/GitHub/Reddit_MDA/RedditTaggerFinal150.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing functions.\n",
    "\n",
    "Here the following functions are defined:\n",
    "\n",
    "Check English:\n",
    "\n",
    "    (Calculates what % of all words in a piece of text are English.\n",
    "    Parameter text is a string, parameter cutoff a float between 0 and 1.\n",
    "    Returns either True or False depending on % of English words in the text,\n",
    "    so it can be used as part of an if-statement)\n",
    "\n",
    "Open reddit json:\n",
    "\n",
    "    (Takes Reddit json file. Separates each sentence into one dictionary.  \n",
    "    Simplifies metainfo (retains body, author, link_id, subreddit). \n",
    "    Removes deleted and non-English comments. \n",
    "    Returns dict of dicts in format {id: {body: str, author: str, link_id: str, sentence_no: int, subreddit: str, features: dict})\n",
    "\n",
    "Lengthening:\n",
    "\n",
    "    (Takes Reddit json file. Separates each sentence into one dictionary.  \n",
    "    Simplifies metainfo (retains body, author, link_id, subreddit). \n",
    "    Removes deleted and non-English comments. \n",
    "    Returns dict of dicts in format {id: {body: str, author: str, link_id: str, sentence_no: int, subreddit: str, features: dict})\n",
    "\n",
    "Analyze sentence:\n",
    "\n",
    "    (Takes the preprocessed json and adds to the features sub-dictionary the following keys and counts (values): \"hashtag_201\": no. of hashtags,\n",
    "    \"question_208\": no. of question marks, \"exclamation_209\": no of exclamation marks, \"lenchar_210\": len of sentence in char, \"lenword_211\": len of sentence in words, \n",
    "    \"conjuncts_045\", \"reddit_vocab)\n",
    "\n",
    "Clean sentence:\n",
    "\n",
    "    (Takes a sentence and returns it in all lowercase, with punctuation removed, and emojis removed.)\n",
    "\n",
    "Tag sentence:\n",
    "\n",
    "    (Takes a sentence, cleans it with clean_sentence, and tags it using the NLTK averaged_perceptron_tagger or the FLAIR tagger. Adds a look ahead/behind buffer of three items of type (\"X\", \"X\") to prevent negative indices and IndexErrors\n",
    "    Returns a list of tuples of (word, pos_tag).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_lower = set(word.lower() for word in nltk.corpus.words.words('en')) # List of English words for check_English()\n",
    "\n",
    "def check_English(text):\n",
    "    words = [x.lower().strip(string.punctuation) for x in text.split() if x.strip(string.punctuation)]\n",
    "    eng_words = [x for x in words if x in eng_vocab_lower]\n",
    "    if len(words) > 0:\n",
    "        perc_eng = len(eng_words)/len(words) #BUG: I get a divide by 0 error here (KM)\n",
    "        if perc_eng >= 0.4:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def open_reddit_json(filename):\n",
    "    errors = 0\n",
    "    base = os.path.splitext(filename)[0] #strip the .json extension\n",
    "\n",
    "    with open(filename, \"r\", errors=\"replace\") as j:\n",
    "        print(\"Opening file: \" + str(filename))\n",
    "        prepped_json = {}\n",
    "        counter = 0 #counts number of comments, for ID below\n",
    "        for line in j:\n",
    "            counter += 1\n",
    "\n",
    "            try: \n",
    "                if json.loads(line.strip())[\"body\"] != \"[deleted]\" and check_English(json.loads(line.strip())[\"body\"]): #does not consider deleted comments or comments that fail check_English\n",
    "                    body = json.loads(line.strip())[\"body\"]\n",
    "                    author = json.loads(line.strip())[\"author\"]\n",
    "                    link_id = json.loads(line.strip())[\"link_id\"]\n",
    "                    subreddit = json.loads(line.strip())[\"subreddit\"]\n",
    "\n",
    "                sentence_counter = 0\n",
    "                for sentence in nltk.tokenize.sent_tokenize(body): #separates into sentences\n",
    "                    if sentence.strip(string.punctuation): \n",
    "                        sentence_counter +=1 #keep track of which sentence it is (1st, 2nd, etc.)\n",
    "                        sentence_dict = {\"body\": sentence, \"author\": author, \"link_id\": link_id, \"sentence_no\": sentence_counter, \"subreddit\": subreddit}\n",
    "                        sentence_dict[\"features\"] = s.copy()\n",
    "                        prepped_json[str(base + \"_\" + str(link_id) + \"_\" + str(sentence_counter))] = sentence_dict #creates a dict within a dict, so that the key (filename, linkid, sentence number) calls the whole dict\n",
    "\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                errors +=1 #keeps track of how many errors are encountered/lines skipped\n",
    "\n",
    "        print(\"Total lines skipped = \" + str(errors))\n",
    "    return prepped_json\n",
    "\n",
    "def lengthening(word):\n",
    "    count = 1\n",
    "    character = \"\"\n",
    "    for c in word:\n",
    "        if c == character and not c == \"w\":\n",
    "            count += 1\n",
    "            if count == 3:\n",
    "                return(True)\n",
    "        else:\n",
    "            count = 1\n",
    "            character = c\n",
    "    return(False)\n",
    "\n",
    "# Untagged feature extraction functions\n",
    "def analyze_sentence(preprocessed_json):\n",
    "    # AB: General comment: careful with the .count() function. It has no inherent concept of word boundaries, which will lead to false positives in some cases (see below)\n",
    "\n",
    "    for id in preprocessed_json: \n",
    "        sentence_dict = preprocessed_json.get(id)\n",
    "        sentence = sentence_dict[\"body\"].lower() # AB: lowercasing spelling here, as most code below presupposes all lowercase.\n",
    "        # AB: For individual items in the .count() functions, there is a tradeoff between \" ITEM \" and \"ITEM\".\n",
    "        # AB: Without spaces, there may be unexpected false positives (e.g. \"such a\" counting \"such astronomical costs\" etc.)\n",
    "        # AB: With spaces, we lose items in sentence-initial position and with following puncutation, e.g. \"for instance,\"\n",
    "        # AB: I have made decisions on an educated-guess case basis here, and they fall into three types:\n",
    "        # AB: a) keep \"ITEM\" because false positives are extremely unlikely (e.g. \"in other words\").\n",
    "        # AB: b) insert spaces, because \" ITEM \" is going to catch all relevant cases (e.g. \" such a \" \n",
    "        s = sentence_dict[\"features\"]\n",
    "\n",
    "        s[\"hashtag_201\"] = len(re.findall(r\"#\\w+\", sentence)) # AB: Used a regex here because otherwise sequences of \"#\" or individual \"#\" are going to inflate the count\n",
    "\n",
    "        if sentence.endswith(\"?\"):\n",
    "            s[\"question_208\"] += 1\n",
    "            \n",
    "        if sentence.endswith(\"!\"):\n",
    "            s[\"exclamation_209\"] += 1\n",
    "        \n",
    "        s[\"emojis_218\"] = adv.extract_emoji(sentence)[\"overview\"][\"num_emoji\"]\n",
    "        \n",
    "        for emphatic in [\" for sure\"]: \n",
    "            s[\"amplifiers_048\"] += sentence.count(emphatic)\n",
    "        if sentence.startswith(\"for sure\"): # AB: Catch cases of sentence-initial \"for sure\" that have been excluded through the insertion of spaces above\n",
    "            s[\"amplifiers_048\"] += 1\n",
    "\n",
    "        for hedge in [\" at about \", \" something like \", \" more or less\", \" kinda \", \" sorta \", \" almost \", \" maybe \"]:\n",
    "            s[\"hedges_047\"] += sentence.count(hedge)\n",
    "        if sentence.startswith(\"at about \") or sentence.startswith(\"something like \") or sentence.startswith(\"more or less\") or sentence.startswith(\"kinda \") or sentence.startswith(\"sorta \"):\n",
    "        # AB: Catch sentence-initial cases excluded by spaces above\n",
    "            s[\"hedges_047\"] += 1\n",
    "\n",
    "        for conjunct in [\"on the contrary\", \"on the other hand\", \"for example\", \"for instance\", \"by contrast\", \"by comparison\", \"in comparison\",\n",
    "                         \"in contrast\", \"in particular\", \"in addition\", \"in conclusion\", \"in consequence\", \"in sum\", \"in summary\", \"in any event\",\n",
    "                         \"in any case\", \"in other words\", \"as a result\", \"as a consequence\"]:\n",
    "            s[\"conjuncts_045\"] += sentence.count(conjunct) # AB: I have not inserted any spaces above because the likelihood of false positives in all cases seems very low\n",
    "\n",
    "        for conjunct in [\"that is,\", \"else,\", \"altogether,\", \"rather,\"]: #Will only catch sentences with proper punctuation but it's a start\n",
    "            if sentence.startswith(conjunct):\n",
    "                s[\"conjuncts_045\"] += 1\n",
    "\n",
    "        for advsub in [\"inasmuch as\", \"forasmuch as\", \"insofar as\", \"insomuch as\", \" as long as \", \" as soon as \"]:\n",
    "            s[\"advsubother_038\"] += sentence.count(advsub)\n",
    "        if sentence.startswith(\"as long as \") or sentence.startswith(\"as soon as \"):\n",
    "            s[\"advsubother_038\"] += 1\n",
    "            \n",
    "        for advsubcond in [\" if \", \" unless \", \" if, \", \" unless, \"]:\n",
    "            s[\"advsubcond_037\"] += sentence.count(advsubcond)\n",
    "        if sentence.startswith(\"if \") or sentence.startswith(\"if, \") or sentence.startswith(\"unless \") or sentence.startswith(\"unless, \"):\n",
    "            s[\"advsubcond_037\"] += 1\n",
    "        s[\"lenchar_210\"] = len(sentence) \n",
    "        s[\"lenword_211\"] = len(sentence.split(\" \")) \n",
    "        \n",
    "        for emoticon in [\":-)\", \":)\", \";-)\", \":-P\", \";-P\", \":-p\", \";-p\", \":-(\", \";-(\", \":-O\", \"^^\", \"-.-\", \":-$\", \":-\\\\\", \":-/\", \":-|\", \";-/\", \";-\\\\\",\n",
    "                        \":-[\", \":-]\", \":-ยง\", \"owo\", \"*.*\", \";)\", \":P\", \":p\", \";P\", \";p\", \":(\", \";(\", \":O\", \":o\", \":|\", \";/\", \";\\\\\", \":[\", \":]\", \":ยง\"]:\n",
    "            s[\"emoticons_207\"] += sentence.count(emoticon)\n",
    "            ## here: enter command to replace emojis. Otherwise they will be split and the letter will most likely be tagged as NOUN, which throws off some of the functions below.\n",
    "            ### AB: This is not the place to do it, as whatever we do here will not persist into what is being piped to the tokenizer.\n",
    "            ### AB: I added the operation to the clean_sentence() function\n",
    "\n",
    "        words = sentence_dict[\"body\"].split() #split into words for single word functions below\n",
    "        \n",
    "        sum_wordlen = 0\n",
    "        for word in words:\n",
    "            word = re.sub(r'[^\\w\\s]','', word)\n",
    "            wordlen = len(word)\n",
    "            sum_wordlen = sum_wordlen + wordlen\n",
    "        s[\"wordlength_044\"] = (sum_wordlen/len(words)) \n",
    "        # this works fine but the output might look weird since the words are here separated differently than they are by the tagger\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            if lengthening(words[i].lower()):\n",
    "                s[\"lengthening_206\"] += 1\n",
    "            if words[i].lower() in [\"op\", \"subreddit\", \"sub\", \"subreddits\", \"upvoted\", \"posted\", \"repost\", \"thread\", \"upvotes\", \"upvote\", \"upvoting\"\n",
    "                    \"reddit\", \"redditor\", \"redditors\", \"post\", \"posts\", \"mod\", \"mods\", \"flair\", \"karma\", \"downmod\", \"downmodding\", \"downvote\", \n",
    "                    \"downvoting\", \"modding\"]:\n",
    "                s[\"reddit_vocab_216\"] += 1 \n",
    "            \n",
    "            if words[i].lower().startswith(\"u/\"):\n",
    "                s[\"link_202\"] += 1 \n",
    "                words[i] = \"username\" ## added these replacement statements to ease the later processing - and also for anonymisation (HM)\n",
    "                \n",
    "            if words[i].lower().startswith(\"r/\"):\n",
    "                s[\"link_202\"] += 1 \n",
    "                words[i] = \"subredditname\" ## added these replacement statements to ease the later processing (HM)\n",
    "\n",
    "            if \"http\" in words[i].lower() or \"www\" in words[i].lower():\n",
    "                s[\"interlink_203\"] += 1 \n",
    "                words[i] = \"url\" ## added these replacement statements to ease the later processing (HM)\n",
    "\n",
    "            if not i == 0:\n",
    "                if words[i].isupper() and not words[i]==\"I\":\n",
    "                    s[\"caps_204\"] += 1\n",
    "            else:\n",
    "                if words[i].isupper() and not (words[i] in [\"A\", \"I\"]): \n",
    "                    s[\"caps_204\"] += 1  \n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = str(sentence).strip(string.punctuation).lower()\n",
    "    for emoticon in [\":-)\", \":)\", \";-)\", \":-P\", \";-P\", \":-p\", \";-p\", \":-(\", \";-(\", \":-O\", \"^^\", \"-.-\", \":-$\", \":-\\\\\", \":-/\", \":-|\", \";-/\", \";-\\\\\",\n",
    "                        \":-[\", \":-]\", \":-ยง\", \"owo\", \"*.*\", \";)\", \":P\", \":p\", \";P\", \";p\", \":(\", \";(\", \":O\", \":o\", \":|\", \";/\", \";\\\\\", \":[\", \":]\", \":ยง\"]:\n",
    "        sentence = sentence.replace(emoticon, \"\")\n",
    "    ## emoticons already counted (but not removed) in the analyse_sentence function\n",
    "    ## emojis already counted (but not removed) in the analyse_sentence function\n",
    "    ## links and URLs counted AND removed in the analyse_sentence function\n",
    "    return sentence    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the function for tagging sentences with NLTK:\n",
    "\n",
    "(Takes a sentence, cleans it with clean_sentence, and tags it using the NLTK averaged_perceptron_tagger. \n",
    "    Adds a look ahead/behind buffer of three items of type (\"X\", \"X\") to prevent negative indices and IndexErrors\n",
    "    Returns a list of tuples of (word, pos_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentence(sentence):\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "    tokens = nltk.word_tokenize(cleaned_sentence)\n",
    "    tagged_sentence = nltk.pos_tag(tokens)\n",
    "    empty_look = [(\"X\", \"X\"), (\"X\", \"X\"), (\"X\", \"X\")]\n",
    "    tagged_sentence = empty_look + tagged_sentence + empty_look \n",
    "    return tagged_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the function for tagging sentences with FLAIR:\n",
    "\n",
    "(Takes a sentence, cleans it with clean_sentence, and tags it using the FLAIR POS tagger. \n",
    "    Adds a look ahead/behind buffer of three items of type (\"X\", \"X\") to prevent negative indices and IndexErrors\n",
    "    Returns a list of tuples of (word, pos_tag).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentence(sentence):\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "    flair_sentence = Sentence(cleaned_sentence)\n",
    "    tagger_FLAIR.predict(flair_sentence)\n",
    "    token_list = []\n",
    "    for label in flair_sentence.get_labels('pos'):\n",
    "        token_list.append(tuple([label.data_point.text] + [label.value])) \n",
    "    empty_look = [(\"X\", \"X\"), (\"X\", \"X\"), (\"X\", \"X\")]\n",
    "    tagged_sentence = empty_look + token_list + empty_look \n",
    "    return tagged_sentence   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of global variables incl. stopword lists and checkword lists for following POS-functions & feature dict\n",
    "\n",
    "The following lists of words are used for extracting the counts of features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {\"vpast_001\": 0, \"vpresperfect_002a\": 0, \"vpastperfect_002b\": 0, \"vpresent_003\": 0, \"advplace_004\": 0, \"advtime_position_005a\": 0, \"advtime_durfreq_005b\": 0, \n",
    "     \"profirpers_006\": 0, \"prosecpers_007\": 0,\"prothirdper_008\": 0, \"proit_009\": 0, \"prodemons_010\": 0, \"proindef_011\": 0, \n",
    "     \"pverbdo_012\": 0, \"whquest_013\": 0, \"nominalis_014\": 0, \"gerund_015\": 0,\"nouns_016\": 0, \"passagentl_017\": 0, \"passby_018\": 0, \n",
    "     \"mainvbe_019\": 0, \"exthere_020\": 0, \"thatvcom_021\": 0, \"thatacom_022\": 0, \"whclause_023\": 0, \"vinfinitive_024\": 0, \n",
    "     \"vpresentpart_025\": 0, \"vpastpart_026\": 0, \"vpastwhiz_027\": 0, \"vpresentwhiz_028\":0, \"thatresub_029\": 0, \"thatreobj_030\": 0, \n",
    "     \"whresub_031\": 0, \"whreobj_032\": 0, \"whrepied_033\": 0, \"sentencere_034\": 0, \"advsubcause_035\": 0, \"advsubconc_036\": 0, \n",
    "     \"advsubcond_037\": 0, \"advsubother_038\": 0, \"prepositions_039\": 0, \"adjattr_040\": 0, \"adjpred_041\": 0, \"adverbs_042\": 0, \n",
    "     \"ttratio_043\": 0, \"wordlength_044\": 0, \"conjuncts_045\": 0, \"downtoners_046\": 0, \"hedges_047\": 0, \"amplifiers_048\": 0, \n",
    "     \"discpart_050\": 0, \"demonstr_051\": 0, \"modalsposs_052\": 0, \"modalsness_053\": 0, \"modalspred_054\": 0, \n",
    "     \"vpublic_055\": 0, \"vprivate_056\": 0, \"vsuasive_057\": 0, \"vseemappear_058\": 0, \"contractions_059\": 0, \"thatdel_060\": 0, \n",
    "     \"strandprep_061\": 0, \"vsplitinf_062\": 0, \"vsplitaux_063\": 0, \"coordphras_064\": 0, \"coordnonp_065\": 0, \"negsyn_066\": 0, \n",
    "     \"negana_067\": 0, \"hashtag_201\": 0, \"link_202\": 0, \"interlink_203\": 0, \"caps_204\": 0, \"vimperative_205\": 0, \"lengthening_206\":0,\n",
    "     \"emoticons_207\":0, \"question_208\": 0, \"exclamation_209\": 0, \"lenchar_210\": 0, \"lenword_211\": 0, \"comparatives_syn_212\": 0, \n",
    "     \"superlatives_syn_213\": 0, \"comparatives_ana_214\": 0, \"superlatives_ana_215\":0, \"reddit_vocab_216\":0, \"vprogressive_217\": 0,\n",
    "     \"emojis_218\":0}\n",
    "placelist = [\"aboard\", \"above\", \"abroad\", \"across\", \"ahead\", \"alongside\", \"anywhere\", \n",
    "                 \"ashore\", \"astern\", \"away\", \"behind\", \"below\", \"beneath\", \"between\", \"beyond\",\n",
    "                 \"beside\", \"down\", \"downhill\", \"downstairs\", \"downstream\", \"downwind\", \"east\",\n",
    "                 \"eastward\", \"eastwards\", \"elsewhere\", \"everywhere\", \"far\", \"here\", \"hereabouts\",\n",
    "                 \"indoors\", \"inland\", \"inshore\", \"inside\", \"locally\", \"near\", \"nearby\", \"north\",\n",
    "                 \"northward\", \"northwards\", \"nowhere\", \"offshore\", \"opposite\", \"outdoors\", \"outside\", \n",
    "                 \"overboard\", \"overhead\", \"overland\", \"overseas\", \"somewhere\", \"south\", \"southward\", \"southwards\",\n",
    "                 \"there\", \"thereabouts\", \"through\", \"throughout\", \"under\", \"underfoot\", \"underground\", \"underneath\",\n",
    "                 \"uphill\", \"upstairs\", \"upstream\", \"west\", \"westward\", \"westwards\", \"within\"] \n",
    "timepoints = [\"afterwards\", \"again\", \"already\", \"anymore\", \"before\", \"currently\", \"earlier\", \"early\", \"eventually\",\n",
    "              \"formerly\", \"finally\", \n",
    "                \"immediately\", \"initially\", \"instantly\", \"late\", \"lately\", \"later\", \"momentarily\", \n",
    "                \"now\", \"nowadays\",  \"originally\", \"presently\", \"previously\", \"promptly\", \"recently\", \n",
    "                \"shortly\", \"simultaneously\", \"soon\", \"subsequently\", \"today\", \"tomorrow\", \"tonight\",\n",
    "                \"yesterday\"]\n",
    "timedurfreq = [\"always\", \"annually\", \"ceaselessly\", \"commonly\", \"constantly\", \"continually\", \"continuously\", \"customarily\",\n",
    "               \"daily\", \"eternally\", \"evermore\", \"endlessly\", \"forever\", \"fortnightly\", \"frequently\", \"habitually\", \"hourly\", \"infrequently\", \"intermittently\",\n",
    "               \"irregularly\", \"invariably\", \"monthly\", \"never\", \"occasionally\", \"often\", \"oftentimes\", \"once\", \"periodically\",\n",
    "               \"perpetually\", \"persistently\", \"rarely\", \"repeatedly\", \"routinely\", \"seldom\", \"sometimes\",\n",
    "               \"twice\", \"unceasingly\", \"usually\",\"weekly\", \"yearly\"]\n",
    "firstpersonlist = [\"i\", \"me\", \"we\", \"us\", \"my\", \"our\", \"myself\", \"ourselves\"]\n",
    "secondpersonlist = [\"you\", \"yourself\", \"your\", \"yourselves\"]\n",
    "thirdpersonlist = [\"she\", \"he\", \"they\", \"her\", \"him\", \"them\", \"his\", \"their\", \"himself\",\"herself\", \"themselves\"]\n",
    "indefpronounlist = [\"anybody\", \"anyone\", \"anything\", \"everybody\", \"everyone\", \"everything\", \"nobody\", \"none\", \"nothing\", \"nowhere\", \"somebody\", \"someone\", \"something\"]\n",
    "conjunctslist = [\"alternatively\", \"altogether\", \"consequently\", \"conversely\", \"eg\", \"e.g.\", \"else\", \"furthermore\",\n",
    "                 \"hence\", \"however\", \"ie\", \"i.e.\", \"instead\", \"likewise\", \"moreover\", \"namely\", \"nevertheless\",\n",
    "                 \"nonetheless\", \"notwithstanding\", \"otherwise\", \"rather\", \"similarly\", \"therefore\", \"thus\", \"viz\"]\n",
    "conjunctsmultilist = [\"on the contrary\", \"on the other hand\", \"for example\", \"for instance\", \"by contrast\", \"by comparison\", \n",
    "                      \"in comparison\", \"in contrast\", \"in particular\", \"in addition\", \"in conclusion\", \"in consequence\", \"in sum\",\n",
    "                      \"in summary\", \"in any event\", \"in any case\", \"in other words\", \"as a result\", \"as a consequence\"]\n",
    "punct_final = [\".\", \"!\", \"?\", \":\", \";\"] # here, Biber also includes the long dash -- , but I am unsure how this would be rendered\n",
    "belist = [\"be\", \"am\", \"are\", \"is\", \"was\", \"were\", \"been\", \"being\", \"'m\", \"'re\",] # I have added the contracted forms of am and are (AB)\n",
    "havelist = [\"have\", \"has\", \"had\", \"having\"]\n",
    "dolist = [\"do\", \"does\", \"doing\", \"did\", \"done\"]\n",
    "subjpro = [\"i\", \"we\", \"he\", \"she\", \"they\"]\n",
    "posspro = [\"my\", \"our\", \"your\", \"his\", \"their\", \"its\"]\n",
    "DEM = [\"that\", \"this\", \"these\", \"those\"]\n",
    "WHP = [\"who\", \"whom\", \"whose\", \"which\"]\n",
    "WHO = [\"what\", \"where\", \"when\", \"how\", \"whether\", \"why\", \"whoever\", \"whomever\", \"whichever\", \n",
    "       \"whenever\", \"whatever\", \"however\"] \n",
    "discpart = [\"well\", \"now\", \"anyway\", \"anyhow\", \"anyways\", \"though\"]\n",
    "QUAN = [\"each\", \"all\", \"every\", \"many\", \"much\", \"few\", \"several\", \"some\", \"any\"]\n",
    "QUANPRO = [\"everybody\", \"somebody\", \"anybody\", \"everyone\", \"someone\", \"anyone\", \"everything\", \"something\", \"anything\", \"anywhere\"]\n",
    "ALLP = [\".\", \"!\", \"?\", \":\", \";\", \",\"]  # here, Biber also includes the long dash -- , but I am unsure how this would be rendered \n",
    "downtonerlist = [\"almost\", \"barely\", \"hardly\", \"merely\", \"mildly\", \"nearly\", \"only\", \"partially\", \"partly\", \"practically\", \"scarcely\", \"slightly\", \"somewhat\"]\n",
    "                # some others that could be included: a little, a bit, a tad (HM)\n",
    "amplifierlist = [\"absolutely\", \"altogether\", \"completely\", \"definitely\", \"enormously\", \"entirely\", \"extremely\", \"fully\", \"greatly\", \"highly\", \n",
    "                 \"intensely\", \"perfectly\", \"strongly\", \"thoroughly\", \"totally\", \"utterly\", \"very\"]\n",
    "asktelllist = [\"ask\", \"asked\", \"asking\", \"asks\", \"tell\", \"telling\", \"tells\", \"told\"]\n",
    "titlelist = [\"mr\", \"ms\", \"mrs\", \"prof\", \"professor\", \"dr\", \"sir\"]\n",
    "otheradvsublist = [\"since\", \"while\", \"whilst\", \"whereupon\", \"whereas\", \"whereby\", \"such that\", \"so that\", \"such that\", \"inasmuch as\", \"forasmuch as\", \"insofar as\", \"insomuch as\", \"as long as\", \"as soon as\"]\n",
    "notgerundlist = [\"nothing\", \"everything\", \"something\", \"anything\", \"thing\", \"things\", \"string\", \"strings\"]\n",
    "publiclist = [\"acknowledege\", \"acknowledges\", \"acknowledged\", \"acknowledging\", \"admit\", \"admits\", \"admitted\", \"admitting\",\n",
    "              \"agree\", \"agrees\", \"agreed\", \"agreeing\", \"assert\", \"asserts\", \"asserted\", \"asserting\", \"claim\", \"claimed\", \n",
    "              \"claims\", \"claiming\", \"complain\", \"complains\", \"complained\", \"complaining\", \"declare\", \"declared\", \"declares\",\n",
    "              \"declaring\", \"deny\", \"denies\", \"denied\", \"denying\", \"explain\", \"explains\", \"explained\", \"explaining\", \"hint\",\n",
    "              \"hints\", \"hinted\", \"hinting\", \"insist\", \"insisted\", \"insists\", \"insisting\", \"mention\", \"mentions\", \"mentioned\",\n",
    "              \"mentioning\", \"proclaim\", \"proclaims\", \"proclaimed\", \"proclaiming\", \"promise\", \"promises\", \"promised\", \"promising\",\n",
    "              \"protest\", \"protests\",\"protested\", \"protesting\", \"remark\", \"remarks\", \"remarking\", \"remarked\", \"reply\", \n",
    "              \"replied\", \"replies\", \"replying\", \"report\", \"reports\", \"reported\", \"reporting\", \"say\", \"says\", \"said\", \"saying\",\n",
    "              \"suggest\", \"suggests\", \"suggested\", \"suggesting\", \"swear\", \"swears\", \"swore\", \"swearing\", \"write\", \"wrote\", \"writing\", \"writes\"]\n",
    "privatelist = [\"anticipate\", \"anticipates\", \"anticipated\", \"anticipating\", \"assume\", \"assumes\", \"assumed\", \"assuming\",\n",
    "               \"believe\", \"believes\", \"believed\", \"believing\", \"conclude\", \"concludes\", \"concluded\", \"concluding\", \"decide\",\n",
    "               \"decides\", \"decided\", \"deciding\", \"demonstrate\", \"demostrates\", \"demonstrated\",\"demonstrating\", \"determine\",\n",
    "               \"determines\", \"determined\", \"determining\", \"discover\", \"discovers\", \"discovered\", \"discovering\", \"doubt\",\n",
    "               \"doubts\", \"doubted\", \"doubting\", \"estimate\", \"estimated\", \"estimates\", \"estimating\", \"fear\", \"fears\", \"feared\",\n",
    "               \"fearing\", \"feel\", \"feels\", \"feeled\", \"feeling\", \"find\", \"finds\", \"found\", \"finding\", \"forget\", \"forgets\", \n",
    "               \"forgot\", \"forgetting\", \"guess\", \"guesses\", \"guessed\", \"guessing\", \"hear\", \"hears\", \"heard\", \"hearing\", \"hope\",\n",
    "               \"hopes\", \"hoped\", \"hoping\", \"imagine\", \"imagines\", \"imagined\", \"imagining\", \"imply\", \"implies\", \"implied\", \n",
    "               \"implying\", \"indicate\", \"indicates\", \"indicating\", \"indicated\", \"infer\", \"infers\", \"infered\", \"inferring\", \"inferred\",\n",
    "               \"know\", \"knows\", \"knew\", \"knowing\", \"learn\", \"learns\", \"learnt\", \"learned\", \"learning\", \"mean\", \"means\", \"meant\",\n",
    "               \"meaning\", \"notice\", \"notices\", \"noticed\", \"noticing\", \"prove\", \"proves\", \"proved\", \"proving\", \"realise\", \"realize\",\n",
    "               \"realised\", \"realized\", \"realises\",\"realizes\", \"realising\", \"realizing\", \"recognise\", \"recognize\", \"recognises\",\n",
    "               \"recognizes\", \"recognised\", \"recognized\", \"recognising\", \"recognizing\", \"remember\", \"remembers\", \"remembered\",\n",
    "               \"remembering\", \"reveal\", \"reveals\", \"revealing\", \"revealed\", \"see\", \"sees\", \"saw\", \"seen\", \"seeing\", \"show\", \"shows\",\n",
    "               \"showed\", \"showing\", \"suppose\", \"supposed\", \"supposes\", \"supposing\", \"think\", \"thinks\", \"thought\", \"thinking\",\n",
    "               \"understand\", \"understands\", \"understood\", \"understanding\"]\n",
    "suasivelist = [\"agree\", \"agrees\", \"agreed\", \"agreeing\", \"arrange\", \"arranges\", \"arranged\", \"arranging\", \"ask\", \"asks\", \"asked\",\n",
    "               \"asking\", \"beg\", \"begs\", \"begged\", \"begging\", \"command\", \"commands\", \"commanded\", \"commanding\", \"decide\", \"decides\",\n",
    "               \"decided\", \"deciding\", \"demand\", \"demands\", \"demanding\", \"demanded\", \"grant\", \"grants\", \"granted\", \"granting\",\n",
    "               \"insist\", \"insists\", \"insisted\", \"insisting\", \"instruct\", \"instructs\", \"instructed\", \"instructing\", \"ordain\", \n",
    "               \"ordains\", \"ordained\", \"ordaining\", \"pledge\", \"pledges\", \"pledging\", \"pledged\", \"pronounce\", \"pronounces\", \n",
    "               \"pronounced\", \"pronouncing\", \"propose\", \"proposes\", \"proposed\", \"proposing\", \"recommend\", \"recommends\", \"recommended\",\n",
    "               \"recommending\", \"request\", \"requests\", \"requested\", \"requesting\", \"stipulate\", \"stipulates\", \"stipulated\", \"stipulating\",\n",
    "               \"suggest\", \"suggests\", \"suggested\", \"suggesting\", \"urge\", \"urged\", \"urges\", \"urging\"]\n",
    "copulalist = [\"be\", \"am\", \"is\", \"was\", \"were\", \"been\", \"being\", \"appear\", \"appears\", \"appeared\", \"appearing\", \"seem\", \"seems\", \"seemed\", \"seeming\", \n",
    "              \"sound\", \"sounds\", \"sounding\", \"sounded\", \"smell\", \"smells\", \"smelled\", \"smelling\", \"become\", \"becomes\", \"became\", \"becoming\", \"turn\", \n",
    "              \"turns\", \"turning\", \"turned\", \"turn\", \"grow\", \"grows\", \"grew\", \"growing\", \"growed\", \"grown\", \"get\", \"gets\", \"getting\", \"gotten\", \n",
    "              \"got\", \"look\", \"looks\", \"looking\", \"looked\", \"taste\", \"tastes\", \"tasted\", \"tasting\", \"feel\", \"feels\", \"feeled\", \"felt\", \"feeling\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech functions. The following functions are defined below:\n",
    "\n",
    "Analyze verb:\n",
    "\n",
    "(Takes the index position of the current word, a tagged sentence, and dictionary of all possible tags and updates relevant keys: \"vpast_001\", \"vpresperfect_002a\", \"vpastperfect_002b\", \"vpresent_003\", \"pverbdo_012\", \"passagentl_017\", \"passby_018\", \"mainvbe_019\", \"whclause_023\", \"vinfinitive_024\", \"vpresentpart_025\", \"vpastpart_026\", \"vpastwhiz_027\", \"vpresentwhiz_028\", \"vpublic_055\", \"vprivate_056\", \"vsuasive_057\", \"vseemappear_058\", \"contractions_059\", \"thatdel_060\", \"vsplitinf_062\", \"vsplitaux_063\", \"vimperative_205\")\n",
    "\n",
    "Analyze modal:\n",
    "\n",
    "(Takes the index position of the current word, a tagged sentence, and dictionary of all possible tags and updates relevant keys: \"modalsposs_052\", \"modalsness_053\", \"modalspred_054\", \"contractions_059\", \"vsplitaux_063\".)\n",
    "\n",
    "Analyze adverb:\n",
    "\n",
    "(Takes the index position of the current word, a tagged sentence, and dictionary of all possible tags and updates relevant keys: \"advplace_004\", \"advtime_005\", \"adverbs_042\", \"conjuncts_045\", \"downtoners_046\", \"hedges_047\", \"amplifiers_048\", \"discpart_050\", \"contractions_059\", \"negana_067\".)\n",
    "\n",
    "Analyze adjective:\n",
    "\n",
    "(Takes the index position of the current word, a tagged sentence, and dictionary of all possible tags and updates relevant keys: \"adjattr_040\", \"adjpred_041\", \"comparatives_212\", \"superlatives_213\".)\n",
    "\n",
    "Analyze preposition:\n",
    "\n",
    "(Takes the index position of the current word, a tagged sentence, and dictionary of all possible tags and updates relevant keys: \"advsubcause_035\", \"advsubconc_036\", \"advsubcond_037\", \"advsubother_038\", \"prepositions_039\", \"conjuncts_045\", \"hedges_047\", \"strandprep_061\".)\n",
    "\n",
    "Analyze noun: \n",
    "\n",
    "(Takes the index position of the current word, a tagged sentence, and dictionary of all possible tags and updates relevant keys: \"nominalis_014\", \"gerund_015\", \"nouns_016\")\n",
    "\n",
    "Analyze pronoun:\n",
    "\n",
    "(Takes the index position of the current word, a tagged sentence, and dictionary of all possible tags and updates relevant keys: \"nominalis_014\", \"gerund_015\", \"nouns_016\")\n",
    "\n",
    "Analyze conjunction:\n",
    "\n",
    "(Takes the index position of the current word, a tagged sentence, and dictionary of all possible tags and updates relevant keys: \"hedges_047\", \"coordphras_064\", \"coordnonp_065\".)\n",
    "\n",
    "Analyze determiner:\n",
    "\n",
    "(Takes the index position of the current word, a tagged sentence, and dictionary of all possible tags and updates relevant keys: \"demonstr_051\", \"negsyn_066\".)\n",
    "\n",
    "Analyze wh-word:\n",
    "\n",
    "(Takes the index position of the current word, a tagged sentence, and dictionary of all possible tags and updates relevant keys: \"demonstr_051\", \"negsyn_066\".)\n",
    "\n",
    "Analyze there:\n",
    "\n",
    "(Takes the index position of the current word, a tagged sentence, and dictionary of all possible tags and updates relevant keys: \"exthere_020\".)\n",
    "\n",
    "Analyze particle:\n",
    "\n",
    "(Takes the index position of the current word, a tagged sentence, and dictionary of all possible tags and updates relevant keys: \"exthere_020\".)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_verb(index, tagged_sentence, features_dict):    \n",
    "    word_tuple = tagged_sentence[index]\n",
    "    if word_tuple[1] == \"VBD\":\n",
    "        features_dict[\"vpast_001\"] += 1\n",
    "    elif word_tuple[1] == \"VB\":\n",
    "        if tagged_sentence[index-1][1] == \"X\" or tagged_sentence[index-1][0] == \",\": \n",
    "            features_dict[\"vimperative_205\"] += 1\n",
    "        else: \n",
    "            features_dict[\"vinfinitive_024\"] += 1\n",
    "            if tagged_sentence[index-2][0] == \"to\":\n",
    "                if tagged_sentence[index-1][1] == \"RB\" and not tagged_sentence[index-1][0] in [\"n't\", \"not\"]:\n",
    "                    features_dict[\"vsplitinf_062\"] += 1\n",
    "                else:\n",
    "                    pass\n",
    "            elif tagged_sentence[index-3][0] == \"to\":\n",
    "                if tagged_sentence[index-2][1] == \"RB\" and not tagged_sentence[index-2][0] in [\"n't\", \"not\"]:\n",
    "                    if tagged_sentence[index-1][1] == \"RB\" and not tagged_sentence[index-1][0] in [\"n't\", \"not\"]:\n",
    "                        features_dict[\"vsplitinf_062\"] += 1\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "    elif word_tuple[1] == \"VBG\":\n",
    "        if (tagged_sentence[index-1][1] == \"X\" or tagged_sentence[index-1][0] in ALLP) and tagged_sentence[index+1][1] in [\"IN\", \"DT\", \"RB\", \"WP\",\"PRP\", \"WRB\"]:\n",
    "            features_dict[\"vpresentpart_025\"] += 1\n",
    "        elif tagged_sentence[index-1][1] == \"NN\":\n",
    "            features_dict[\"vpresentwhiz_028\"] += 1 \n",
    "        elif tagged_sentence[index-1][0] in belist:\n",
    "            features_dict[\"vprogressive_217\"] += 1\n",
    "        elif (tagged_sentence[index-2][0] in belist) and (tagged_sentence[index-1][1] == \"RB\"):\n",
    "            features_dict[\"vprogressive_217\"] += 1  \n",
    "            features_dict[\"vsplitaux_063\"] += 1\n",
    "    elif word_tuple[1] == \"VBN\":\n",
    "        if (tagged_sentence[index-1][1] == \"X\" or tagged_sentence[index-1][0] in ALLP):\n",
    "            if tagged_sentence[index+1][1] in [\"IN\", \"RB\", \"TO\"]: # Biber (1988:233) notes for both that \"these forms were edited by hand.\"\n",
    "                features_dict[\"vpastpart_026\"] += 1 ## this one seems accurate enough to me (HM)\n",
    "        elif tagged_sentence[index-1][1] in [\"NN\", \"NNP\"] or tagged_sentence[index-1][0] in QUANPRO:\n",
    "            if tagged_sentence[index+1][1] in [\"IN\", \"RBR\", \"RB\", \"RBS\"] or tagged_sentence[index+1][0] in belist:\n",
    "                features_dict[\"vpastwhiz_027\"] += 1 \n",
    "\n",
    "    elif word_tuple[1] in [\"VBP\",\"VBZ\"]:\n",
    "        features_dict[\"vpresent_003\"] += 1\n",
    "    if word_tuple[0].startswith(\"seem\") or word_tuple[0].startswith(\"appear\"):\n",
    "        features_dict[\"vseemappear_058\"] += 1\n",
    "        \n",
    "    if word_tuple[0] in [\"had\", \"'d\"]: \n",
    "        move_on = True\n",
    "        insert_adv = False\n",
    "        x = index\n",
    "        while move_on:\n",
    "            x += 1\n",
    "            if tagged_sentence[x][1] == \"VBN\":\n",
    "                move_on = False\n",
    "                features_dict[\"vpastperfect_002b\"] += 1\n",
    "                if insert_adv:\n",
    "                    features_dict[\"vsplitaux_063\"] += 1\n",
    "            elif tagged_sentence[x][1].startswith(\"R\") and tagged_sentence[x][0] not in [\"n't\", \"not\"]:\n",
    "                insert_adv = True\n",
    "            elif tagged_sentence[x][1].startswith(\"N\") or tagged_sentence[x][1].startswith(\"P\"):\n",
    "                move_on = True \n",
    "                # HM: as intended by Biber (p. 223) we here also count questions with the word order HAD + NOUN/PRONOUN + VBN\n",
    "                # HM: this does, however, not catch all questions using the past perfect (e.g. does not count \"had many people been to the store?\" due to inserted adjective)\n",
    "            else: \n",
    "                move_on =  False \n",
    "\n",
    "    elif word_tuple[0] in [\"have\", \"'ve\", \"has\"]: \n",
    "        move_on = True\n",
    "        insert_adv = False\n",
    "        x = index\n",
    "        while move_on: \n",
    "            # These while-statements are an attempt to get around the question of how much intervening material to allow by instead setting \n",
    "            # conditions for when to stop looking on (either because an instance of the feature has been found or an impermissible context has been encountered) (AB)\n",
    "            x += 1\n",
    "            if tagged_sentence[x][1] == \"VBN\":\n",
    "                move_on = False\n",
    "                features_dict[\"vpresperfect_002a\"] += 1\n",
    "                if insert_adv:\n",
    "                    features_dict[\"vsplitaux_063\"] += 1\n",
    "            elif tagged_sentence[x][1].startswith(\"R\") and tagged_sentence[x][0] not in [\"n't\", \"not\"]:\n",
    "                insert_adv = True\n",
    "            elif tagged_sentence[x][1].startswith(\"N\") or tagged_sentence[x][1].startswith(\"P\"): \n",
    "                # HM: as intended by Biber (p. 223) we here also count questions with the word order HAVE + NOUN/PRONOUN + VBN\n",
    "                # HM: this does, however, not catch all questions using the present perfect (e.g. does not count \"have many people been to the store?\" due to inserted adjective)\n",
    "                move_on = True\n",
    "            else: \n",
    "                move_on = False\n",
    "                \n",
    "    elif word_tuple[0] in belist:\n",
    "        if tagged_sentence[index+1][1] in [\"DT\", \"PRP$\", \"JJ\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"NNP\"]:\n",
    "            features_dict[\"mainvbe_019\"] += 1\n",
    "        else:\n",
    "            move_on = True\n",
    "            insert_adv = False\n",
    "            x = index\n",
    "            while move_on == True:\n",
    "                x += 1\n",
    "                if tagged_sentence[x][1] == \"VBN\":\n",
    "                    move_on = False\n",
    "                    if tagged_sentence[x+1][0] == \"by\":\n",
    "                        features_dict[\"passby_018\"] += 1\n",
    "                        if insert_adv:\n",
    "                            features_dict[\"vsplitaux_063\"] += 1\n",
    "                    elif tagged_sentence[x+1][1] == \"IN\": # Here, provision is made for by-passive with intervening PPs: \"was shot in the head by an unidentified suspect\"\n",
    "                        x += 1\n",
    "                        move_on2 = True\n",
    "                        while move_on2:\n",
    "                            x += 1\n",
    "                            if tagged_sentence[x+1][1].startswith(\"N\") or tagged_sentence[x+1][1].startswith(\"DT\"): # One might include adjectives here as well, but prob at the cost of precision.\n",
    "                                pass\n",
    "                            elif tagged_sentence[x+1][0] == \"by\":\n",
    "                                features_dict[\"passby_018\"] += 1\n",
    "                                if insert_adv:\n",
    "                                    features_dict[\"vsplitaux_063\"] += 1\n",
    "                                move_on2 = False\n",
    "                            else:\n",
    "                                features_dict[\"passagentl_017\"] += 1\n",
    "                                if insert_adv:\n",
    "                                    features_dict[\"vsplitaux_063\"] += 1\n",
    "                                move_on2 = False\n",
    "                    else:\n",
    "                        features_dict[\"passagentl_017\"] += 1\n",
    "                        if insert_adv:\n",
    "                            features_dict[\"vsplitaux_063\"] += 1  \n",
    "                elif tagged_sentence[x][1].startswith(\"RB\"):\n",
    "                    if tagged_sentence[x][0] not in [\"n't\", \"not\"]:\n",
    "                        insert_adv = True\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    move_on = False\n",
    "    elif word_tuple[0] in dolist:\n",
    "        move_on = True\n",
    "        negator = False\n",
    "        insert_adv = False\n",
    "        x = index\n",
    "        while move_on:\n",
    "            x += 1\n",
    "            if tagged_sentence[x][1].startswith(\"V\"):\n",
    "                move_on = False\n",
    "                if negator == False:\n",
    "                    features_dict[\"amplifiers_048\"] += 1\n",
    "                if insert_adv:\n",
    "                    features_dict[\"vsplitaux_063\"] += 1\n",
    "            elif tagged_sentence[x][0] in [\"not\", \"n't\"]:\n",
    "                negator = True\n",
    "            elif tagged_sentence[x][1].startswith(\"R\"):\n",
    "                insert_adv = True\n",
    "            else:\n",
    "                move_on = False\n",
    "                #if not (tagged_sentence[index-1][0] in WHP+WHO and tagged_sentence[index-2][0] == \"X\"):\n",
    "                    #features_dict[\"pverbdo_012\"] += 1 # This follows the criteria in Biber, but seems too broad. Do we want things like \"do someone a favor\" \"do the boogie\" etc. here? (AB)\n",
    "                        ## HM: this does indeed catch a lot of garbage (\"does this mean\", \"do rides\", \"what he does\" ...)\n",
    "                        ## alternatively we could look for some restricted contexts in which do is sure to be a pro-verb: \n",
    "                        ## - followed by \", too\": I DO, too.\n",
    "                        ## - sentence-final position: I DO.\n",
    "                        ## - followed by a placeholder: DO this/that/it/so.\n",
    "                        ## I implemented this belows. Awaits evaluation from someone else.\n",
    "    if word_tuple[0] in dolist:\n",
    "        if tagged_sentence[index+1][1] == \"X\":\n",
    "            features_dict[\"pverbdo_012\"] += 1\n",
    "        elif tagged_sentence[index+1][0] in [\"too\", \"this\", \"that\", \"it\", \"so\"]:\n",
    "            if tagged_sentence[index+2][1] == \"X\" or tagged_sentence[index+2][0] in ALLP:\n",
    "                features_dict[\"pverbdo_012\"] += 1\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    if word_tuple[0].startswith(\"'\"):\n",
    "        features_dict[\"contractions_059\"] += 1\n",
    "\n",
    "    if word_tuple[0] in publiclist:\n",
    "        features_dict[\"vpublic_055\"] += 1\n",
    "        if tagged_sentence[index + 1][0] in [\"this\", \"these\", \"that\", \"those\", \"I\", \"we\", \"he\", \"she\", \"they\"]:\n",
    "            features_dict[\"thatdel_060\"] += 1\n",
    "        elif tagged_sentence[index + 1][1].startswith(\"NN\") or tagged_sentence[index + 1][1].startswith(\"PR\"):\n",
    "            if tagged_sentence[index + 2][1].startswith(\"V\") or tagged_sentence[index + 2][1] == \"MD\":\n",
    "                features_dict[\"thatdel_060\"] += 1\n",
    "        elif tagged_sentence[index + 1][1] in [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\", \"PRP$\", \"DT\"]:\n",
    "            if tagged_sentence[index + 2][1].startswith(\"NN\"):\n",
    "                if tagged_sentence[index + 3][1].startswith(\"V\") or tagged_sentence[index + 3][1] == \"MD\":\n",
    "                    features_dict[\"thatdel_060\"] += 1\n",
    "        if tagged_sentence[index + 1][0] in WHP or tagged_sentence[index + 1][0] in WHO:\n",
    "            if tagged_sentence[index + 2][1] != \"MD\":\n",
    "                features_dict[\"whclause_023\"] += 1\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    if word_tuple[0] in privatelist:\n",
    "        features_dict[\"vprivate_056\"] += 1\n",
    "        if tagged_sentence[index + 1][0] in [\"this\", \"these\", \"that\", \"those\", \"I\", \"we\", \"he\", \"she\", \"they\"]: ## 60-1 pub/priv/sua + demonstrative pronoun/subjpro (I we he she they)\n",
    "            features_dict[\"thatdel_060\"] += 1\n",
    "        elif tagged_sentence[index + 1][1].startswith(\"NN\") or tagged_sentence[index + 1][1].startswith(\"PR\"): ## 60-2 pub/priv/sua + PRO/N + AUX/V\n",
    "            if tagged_sentence[index + 2][1].startswith(\"V\") or tagged_sentence[index + 2][1] == \"MD\":\n",
    "                features_dict[\"thatdel_060\"] += 1\n",
    "        elif tagged_sentence[index + 1][1] in [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\", \"PRP$\", \"DT\"]: ## 60-3 pub/priv/sua + adj/adv/det/posspro + (Adj) + N + AUX/V\n",
    "            if tagged_sentence[index + 2][1].startswith(\"NN\"):\n",
    "                if tagged_sentence[index + 3][1].startswith(\"V\") or tagged_sentence[index + 3][1] == \"MD\":\n",
    "                    features_dict[\"thatdel_060\"] += 1\n",
    "        if tagged_sentence[index + 1][0] in WHP or tagged_sentence[index + 1][0] in WHO:\n",
    "            if tagged_sentence[index + 2][1] != \"MD\":\n",
    "                features_dict[\"whclause_023\"] += 1\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    if word_tuple[0] in suasivelist:\n",
    "        features_dict[\"vsuasive_057\"] += 1\n",
    "        if tagged_sentence[index + 1][0] in [\"this\", \"these\", \"that\", \"those\", \"I\", \"we\", \"he\", \"she\", \"they\"]: ## 60-1 pub/priv/sua + demonstrative pronoun/subjpro (I we he she they)\n",
    "            features_dict[\"thatdel_060\"] += 1\n",
    "        elif tagged_sentence[index + 1][1].startswith(\"NN\") or tagged_sentence[index + 1][1].startswith(\"PR\"): ## 60-2 pub/priv/sua + PRO/N + AUX/V\n",
    "            if tagged_sentence[index + 2][1].startswith(\"V\") or tagged_sentence[index + 2][1] == \"MD\":\n",
    "                features_dict[\"thatdel_060\"] += 1\n",
    "        elif tagged_sentence[index + 1][1] in [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\", \"PRP$\", \"DT\"]: ## 60-3 pub/priv/sua + adj/adv/det/posspro + (Adj) + N + AUX/V\n",
    "            if tagged_sentence[index + 2][1].startswith(\"NN\"):\n",
    "                if tagged_sentence[index + 3][1].startswith(\"V\") or tagged_sentence[index + 3][1] == \"MD\":\n",
    "                    features_dict[\"thatdel_060\"] += 1\n",
    "        if tagged_sentence[index + 1][0] in WHP or tagged_sentence[index + 1][0] in WHO:\n",
    "            if tagged_sentence[index + 2][1] != \"MD\":\n",
    "                features_dict[\"whclause_023\"] += 1\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "\n",
    "def analyze_modal(index, tagged_sentence, features_dict):\n",
    "    word_tuple = tagged_sentence[index]\n",
    "    if word_tuple[0] in [\"can\",\"may\",\"might\",\"could\"]:\n",
    "        features_dict[\"modalsposs_052\"] += 1\n",
    "    elif word_tuple[0] in [\"ought\",\"should\",\"must\"]:\n",
    "        features_dict[\"modalsness_053\"] += 1\n",
    "    elif word_tuple[0] in [\"will\",\"would\",\"shall\",\"'ll\",\"'d\"]: \n",
    "        features_dict[\"modalspred_054\"] += 1\n",
    "    if word_tuple[0].startswith(\"'\"):\n",
    "        features_dict[\"contractions_059\"] += 1\n",
    "        \n",
    "    move_on = True\n",
    "    insert_adv = False\n",
    "    x = index\n",
    "    while move_on:\n",
    "        x += 1\n",
    "        if tagged_sentence[x][1].startswith(\"V\"):\n",
    "            move_on = False\n",
    "            if insert_adv:\n",
    "                features_dict[\"vsplitaux_063\"] += 1\n",
    "        elif tagged_sentence[x][0] in [\"not\", \"n't\"]:\n",
    "            pass\n",
    "        elif tagged_sentence[x][1].startswith(\"R\"):\n",
    "            insert_adv = True\n",
    "        else:\n",
    "            move_on = False\n",
    "    \n",
    "def analyze_adverb(index, tagged_sentence, features_dict):\n",
    "    features_dict[\"adverbs_042\"] += 1\n",
    "    word_tuple = tagged_sentence[index]\n",
    "    if word_tuple[0] == \"not\":\n",
    "        features_dict[\"negana_067\"] += 1\n",
    "    if word_tuple[0] == \"n't\":\n",
    "        features_dict[\"negana_067\"] += 1\n",
    "        features_dict[\"contractions_059\"] += 1\n",
    "    elif word_tuple[0] in placelist:\n",
    "        features_dict[\"advplace_004\"] += 1\n",
    "    elif word_tuple[0] in timepoints:\n",
    "        features_dict[\"advtime_position_005a\"] += 1\n",
    "    elif word_tuple[0] in timedurfreq:\n",
    "        features_dict[\"advtime_durfreq_005b\"] += 1\n",
    "    elif word_tuple[0] in downtonerlist:\n",
    "        features_dict[\"downtoners_046\"] += 1\n",
    "    elif word_tuple[0] in amplifierlist:\n",
    "        features_dict[\"amplifiers_048\"] += 1 \n",
    "    elif word_tuple[0] in conjunctslist:\n",
    "        features_dict[\"conjuncts_045\"] += 1\n",
    "    elif index == 0 and word_tuple[0] in discpart:\n",
    "        features_dict[\"discpart_050\"] += 1\n",
    " \n",
    "def analyze_adjective(index, tagged_sentence, features_dict):\n",
    "    if tagged_sentence[index][1] == \"JJR\":\n",
    "        features_dict[\"comparatives_syn_212\"] += 1\n",
    "    elif tagged_sentence[index][1] == \"JJS\":\n",
    "        features_dict[\"superlatives_syn_213\"] += 1\n",
    "    if tagged_sentence[index-1][0] == \"more\":\n",
    "        features_dict[\"comparatives_ana_214\"] += 1\n",
    "    elif tagged_sentence[index-1][0] == \"most\":\n",
    "        features_dict[\"superlatives_ana_215\"] += 1\n",
    "        \n",
    "    adj_type = \"attr\"\n",
    "    x = index-1\n",
    "    while adj_type == \"attr\" and tagged_sentence[x][1].startswith(\"R\"):\n",
    "        x -= 1\n",
    "        if tagged_sentence[x][0] in copulalist:\n",
    "            adj_type = \"pred\"\n",
    "    if adj_type == \"attr\":\n",
    "        features_dict[\"adjattr_040\"] += 1\n",
    "    elif adj_type == \"pred\":\n",
    "        features_dict[\"adjpred_041\"] += 1\n",
    "      \n",
    "def analyze_preposition(index, tagged_sentence, features_dict):\n",
    "    word_tuple = tagged_sentence[index]\n",
    "    if not word_tuple[0] in [\"because\", \"unless\", \"whilst\", \"while\", \"though\", \"tho\", \"although\", \"that\", \"since\", \"whereupon\", \"whereas\", \"whereby\"] + timepoints + timedurfreq + placelist: \n",
    "        features_dict[\"prepositions_039\"] += 1 \n",
    "    if word_tuple[0] in [\"because\", \"becuase\", \"beacuse\", \"cause\", \"'cause\", \"cos\", \"'cos\", \"coz\", \"'coz\", \"caus\", \"'caus\", \"cuz\", \"'cuz\", \"bcoz\", \"bcuz\", \"bcos\", \"bcause\", \"bcaus\"] and tagged_sentence[index+1][0] != \"of\":\n",
    "        features_dict[\"advsubcause_035\"] += 1 \n",
    "        # AB: I decided against a separate feature for \"because of\" since it goes into \"prepositions_039\".\n",
    "        # HM: I don't understand what this means. Biber (1988: 236-237) does not list \"because of\" as a preposition (even though it is an obvious contender),\n",
    "        #     and it is purposefully excluded in the list above for \"prepositions_039\". Right now we are not counting \"because of\" at all, are we?\n",
    "    elif word_tuple[0] == \"although\" or word_tuple[0] == \"though\" or word_tuple[0] == \"tho\":\n",
    "        features_dict[\"advsubconc_036\"] += 1 \n",
    "        \n",
    "    elif word_tuple[0] == \"that\" and tagged_sentence[index-1][0] in [\"such\", \"so\"]:\n",
    "        features_dict[\"advsubother_038\"] += 1 \n",
    "\n",
    "    elif word_tuple[0] == \"of\" and tagged_sentence[index-1][0] in [\"kind\", \"sort\"] and not tagged_sentence[index-2][1] in [\"JJ\", \"JJR\", \"JJS\", \"DT\", \"PRP$\"]:\n",
    "        if not tagged_sentence[index-2][0] in [\"what\", \"whatever\", \"whichever\"]:\n",
    "            features_dict[\"hedges_047\"] += 1\n",
    "\n",
    "    if tagged_sentence[index+1][0] in ALLP or tagged_sentence[index+1][1] == \"X\":\n",
    "        features_dict[\"strandprep_061\"] += 1\n",
    "\n",
    "\n",
    "def analyze_noun(index, tagged_sentence, features_dict):\n",
    "    word_tuple = tagged_sentence[index]\n",
    "\n",
    "    if word_tuple[0].endswith(\"ing\") or word_tuple[0].endswith(\"ings\"):\n",
    "        if word_tuple[1] not in notgerundlist:\n",
    "            features_dict[\"gerund_015\"] += 1\n",
    "    elif word_tuple[0].endswith(\"tions\") or word_tuple[0].endswith(\"tion\") or word_tuple[0].endswith(\"ments\") or word_tuple[0].endswith(\"ment\") or word_tuple[0].endswith(\"ness\") or word_tuple[0].endswith(\"ity\") or word_tuple[0].endswith(\"nesses\") or word_tuple[0].endswith(\"ities\"):\n",
    "        features_dict[\"nominalis_014\"] += 1\n",
    "    else: \n",
    "        features_dict[\"nouns_016\"] += 1\n",
    "        \n",
    "def analyze_pronoun(index, tagged_sentence, features_dict):\n",
    "    word_tuple = tagged_sentence[index] #returns a tuple (word, POS)\n",
    "\n",
    "    if word_tuple[0] == \"it\":\n",
    "        features_dict[\"proit_009\"] += 1\n",
    "    elif word_tuple[0] in firstpersonlist:\n",
    "        features_dict[\"profirpers_006\"] += 1\n",
    "    elif word_tuple[0] in secondpersonlist:\n",
    "        features_dict[\"prosecpers_007\"] += 1\n",
    "    elif word_tuple[0] in thirdpersonlist:\n",
    "        features_dict[\"prothirdper_008\"] += 1\n",
    "    elif word_tuple[0] in indefpronounlist:\n",
    "        features_dict[\"proindef_011\"] += 1  \n",
    "\n",
    "    if word_tuple[0] in DEM:\n",
    "        if tagged_sentence[index+1][0] == \"and\":\n",
    "            features_dict[\"prodemons_010\"] += 1\n",
    "        elif tagged_sentence[index+1][1] in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"MD\", \"WP\"]:\n",
    "            features_dict[\"prodemons_010\"] += 1\n",
    "        elif index == (len(tagged_sentence)-1):\n",
    "            features_dict[\"prodemons_010\"] += 1\n",
    "    elif word_tuple[0] == \"that\" and tagged_sentence[index+1][0] == \"'s\":\n",
    "        features_dict[\"prodemons_010\"] += 1\n",
    "\n",
    "def analyze_conjunction(index, tagged_sentence, features_dict):\n",
    "    word_tuple = tagged_sentence[index]\n",
    "\n",
    "    if word_tuple[0] == \"and\": \n",
    "        if tagged_sentence[index-1][1].startswith(\"N\") and tagged_sentence[index+1][1].startswith(\"N\"):\n",
    "            features_dict[\"coordphras_064\"] += 1\n",
    "        elif tagged_sentence[index-1][1].startswith(\"RB\") and tagged_sentence[index+1][1].startswith(\"RB\"):\n",
    "            features_dict[\"coordphras_064\"] += 1\n",
    "        elif tagged_sentence[index-1][1].startswith(\"JJ\") and tagged_sentence[index+1][1].startswith(\"JJ\"):\n",
    "            features_dict[\"coordphras_064\"] += 1\n",
    "        elif tagged_sentence[index-1][1].startswith(\"VB\") and tagged_sentence[index+1][1].startswith(\"VB\"):\n",
    "            features_dict[\"coordphras_064\"] += 1\n",
    "        elif tagged_sentence[index-1][0] == \",\":\n",
    "            if tagged_sentence[index+1][0] in [\"it\", \"so\", \"you\", \"then\"]:\n",
    "                features_dict[\"coordnonp_065\"] += 1\n",
    "            elif tagged_sentence[index+1][1] in subjpro or tagged_sentence[index+1][1] in DEM:\n",
    "                    features_dict[\"coordnonp_065\"] += 1                            \n",
    "            elif tagged_sentence[index+1][0] == \"there\" and tagged_sentence[index+2][0] in belist:\n",
    "                features_dict[\"coordnonp_065\"] += 1\n",
    "        elif tagged_sentence[index-1][0] in punct_final: \n",
    "            features_dict[\"coordnonp_065\"] += 1\n",
    "        elif tagged_sentence[index+1][0] in WHP or tagged_sentence[index+1][0] in WHO or tagged_sentence[index+1][0] in discpart:\n",
    "            features_dict[\"coordnonp_065\"] += 1\n",
    "        elif tagged_sentence[index+1][0] in [\"because\", \"although\", \"though\", \"if\", \"unless\",] or tagged_sentence[index+1][0] in otheradvsublist: # added the otheradvsublist for this particular feature (GK)\n",
    "            features_dict[\"coordnonp_065\"] += 1\n",
    "        elif tagged_sentence[index+1][0] in conjunctslist:\n",
    "            features_dict[\"coordnonp_065\"] += 1\n",
    "\n",
    "\n",
    "    if word_tuple[0] == \"and\" and tagged_sentence[index+1][0] in WHP or tagged_sentence[index+1][0] in WHO:\n",
    "        features_dict[\"coordnonp_065\"] += 1 \n",
    "    elif word_tuple[0] == \"and\" and tagged_sentence[index+1][0] in [\"because\", \"although\", \"though\", \"if\", \"unless\", \"since\", \"while\", \"whilst\", \"whereas\", \"whereby\"]:\n",
    "        features_dict[\"coordnonp_065\"] += 1 \n",
    "    elif word_tuple[0] == \"and\" and tagged_sentence[index+1][0] in discpart:\n",
    "        features_dict[\"coordnonp_065\"] += 1\n",
    "    elif word_tuple[0] == \"and\" and tagged_sentence[index+1][0] in conjunctslist:\n",
    "        features_dict[\"coordnonp_065\"] += 1\n",
    "\n",
    "\n",
    "def analyze_determiner(index, tagged_sentence, features_dict):\n",
    "    word_tuple = tagged_sentence[index]\n",
    "\n",
    "    if word_tuple[0] in DEM:\n",
    "        features_dict[\"demonstr_051\"] += 1\n",
    "    elif word_tuple[0] == \"neither\" or word_tuple[0] == \"nor\":\n",
    "        features_dict[\"negsyn_066\"] += 1\n",
    "    elif word_tuple[0] == \"no\":\n",
    "        if tagged_sentence[index+1][1].startswith(\"NN\") or tagged_sentence[index+1][1].startswith(\"JJ\"):\n",
    "            features_dict[\"negsyn_066\"] += 1\n",
    "        elif tagged_sentence[index+1][0] in QUAN:\n",
    "            features_dict[\"negsyn_066\"] += 1\n",
    "\n",
    "def analyze_wh_word(index, tagged_sentence, features_dict):\n",
    "    # Check: Ft 32 (Biber's way of finding this seems like it could be optimized)\n",
    "    # Check: Ft 22 (catches unintended phrases)\n",
    "    word_tuple = tagged_sentence[index]\n",
    "    \n",
    "    #21 that verb complements (e.g., / said that he went)  \n",
    "    if tagged_sentence[index][0] == \"that\":\n",
    "    # (a) and\\nor\\but\\or\\aho\\ALL-P + that + DET/PRO/there/plural noun/proper noun/TITLE (these are i/zaยฃ-clauses in clause-initial positions)\n",
    "        if tagged_sentence[index-1][0] in ALLP or tagged_sentence[index-1][0] in [\"and\", \"nor\", \"but\", \"or\", \"who\"]:\n",
    "            if tagged_sentence[index+1][1].startswith(\"D\") or tagged_sentence[index+1][1].startswith(\"PR\") or tagged_sentence[index+1][0] == \"there\" or tagged_sentence[index+1][1].startswith(\"NNP\") or tagged_sentence[index+1][1].startswith(\"NNS\") or tagged_sentence[index+1][0] in titlelist:\n",
    "                features_dict[\"thatvcom_021\"] += 1\n",
    "    # (b) PUB/PRV/SUA/SEEM/APPEAR + that + xxx (where xxx is NOT: V/AUX/CL-P/and)\n",
    "        elif (tagged_sentence[index-1][0] in suasivelist) or (tagged_sentence[index-1][0] in privatelist) or (tagged_sentence[index-1][0] in publiclist):\n",
    "            if not tagged_sentence[index+1][1].startswith(\"V\"):\n",
    "                if tagged_sentence[index+1][0] not in [ALLP, \"and\"]:\n",
    "                    features_dict[\"thatvcom_021\"] += 1\n",
    "        elif (tagged_sentence[index-1][0].startswith(\"seem\")) or (tagged_sentence[index-1][0].startswith(\"appear\")):\n",
    "            if not tagged_sentence[index+1][1].startswith(\"V\"):\n",
    "                if tagged_sentence[index+1][0] not in [ALLP, \"and\"]:\n",
    "                    features_dict[\"thatvcom_021\"] += 1            \n",
    "    # (c) PUB/PRV/SUA + PREP + xxx + N + that (where xxx is any number of words, but NOT = N)\n",
    "    #     (This algorithm allows an intervening prepositional phrase between a verb and its complement.)  \n",
    "    ### HM: this is not implemented at the moment, since it seems very complicated and rather marginal. My guess is that we need to trash this whole feature anyway...\n",
    "    \n",
    "    #29. that relative clauses on subject position (e.g., the dog that bit me) N -p (T#) + that + (ADV) + AUX/V {that relatives across intonation boundaries are identified by hand.)\n",
    "    #30. that relative clauses on object position (e.g., the dog that I saw) N + (T#) + that + DET / SUBJPRO / POSSPRO / it / ADJ / plural noun/ proper noun / possessive noun / TITLE\n",
    "        if tagged_sentence[index-1][1].startswith(\"NN\"):\n",
    "            if tagged_sentence[index+1][1].startswith(\"RB\"):\n",
    "                if (tagged_sentence[index+2][1].startswith(\"V\") or tagged_sentence[index+2][1].startswith(\"MD\")):\n",
    "                    features_dict[\"thatresub_029\"] += 1 \n",
    "            elif tagged_sentence[index+1][1].startswith(\"VB\") or tagged_sentence[index+1][1].startswith(\"MD\"):\n",
    "                features_dict[\"thatresub_029\"] += 1\n",
    "\n",
    "            elif tagged_sentence[index+1][1].startswith(\"DT\") or tagged_sentence[index+1][1].startswith(\"JJ\") or tagged_sentence[index+1][1] == \"NNS\" or tagged_sentence[index+1][1].startswith(\"NNP\"):\n",
    "                features_dict[\"thatreobj_030\"] += 1\n",
    "\n",
    "            elif tagged_sentence[index+1][0] == \"it\" or tagged_sentence[index+1][0] in subjpro or tagged_sentence[index+1][0] in posspro:\n",
    "                features_dict[\"thatreobj_030\"] += 1    \n",
    "            \n",
    "        if tagged_sentence[index-1][1].startswith(\"J\"):\n",
    "            features_dict[\"thatacom_022\"] += 1 \n",
    "                \n",
    "    else:\n",
    "        if word_tuple[0] in WHP: # WHP = [\"who\", \"whom\", \"whose\", \"which\"]\n",
    "            if tagged_sentence[index-1][1] == \"IN\":\n",
    "                features_dict[\"whrepied_033\"] += 1 #pied-piping relative clauses (e.g., the manner in which he was told) PREP + WHP in relative clauses\n",
    "\n",
    "            if word_tuple[0] == \"which\" and tagged_sentence[index-1][0] == \",\": #34. sentence relatives (e.g., Bob likes fried mangoes, which is the most disgusting thing I've ever heard of) Biber: (These forms are edited by hand to exclude non-restrictive relative clauses.)\n",
    "                features_dict[\"sentencere_034\"] += 1\n",
    "\n",
    "            #31. WH relative clauses on subject position (e.g., the man who likes popcorn) xxx + yyy + N + WHP + (ADV) + AUX/V (where xxx is NOT any form of the verbs ASK or TELL; to exclude indirect WH questions like Tom asked the man who went to the store)\n",
    "            ##AB: Added the condition for ASK and TELL\n",
    "            if tagged_sentence[index-1][1].startswith(\"N\") and tagged_sentence[index-2][0] not in [\"tell\", \"tells\", \"told\", \"telling\", \"ask\", \"asks\", \"asked\", \"asking\"]:\n",
    "                if tagged_sentence[index+1][1].startswith(\"R\"):\n",
    "                    if (tagged_sentence[index+2][1].startswith(\"V\") or tagged_sentence[index+2][1].startswith(\"MD\")):\n",
    "                        features_dict[\"whresub_031\"] += 1\n",
    "\n",
    "                elif(tagged_sentence[index+1][1].startswith(\"V\") or tagged_sentence[index+1][1].startswith(\"MD\")):\n",
    "                    features_dict[\"whresub_031\"] += 1\n",
    "        \n",
    "            #32. WH relative clauses on object positions (e.g., the man who Sally likes) xxx + yyy + N + WHP + zzz (where xxx is NOT any form of the verbs ASK or TELL, to exclude indirect WH questions, and zzz is not ADV, AUX or V, to exclude relativization on subject position)\n",
    "            #right now, only wh-words at least two words from the front and 2 from the end will be caught here (KM) -> won't catch ex \"boys who Sally likes\" (is that grammatically acceptable??) also won't catch passives, ex \"the men who are liked by Sally\" (kind of awkward tbh) (KM)\n",
    "            #AB: The two words from the end is not a problem, since an object RC has, by definition a minimum of two items after the relativizer (subject and verb)\n",
    "            #AB: I am hard-put to find a more felicitous example than \"boys who Sally likes\" that would pose a problem with the 2 words at the beginning either, so happy to disregard the issue.\n",
    "            #AB: Edit: Does the problem at beginning or end not disappear entirely with out added \"x\"es?\n",
    "            #AB: The example \"the men who are liked by Sally\" is an RC with the relatizive in subject gap and as such is appropriately captured under 031.\n",
    "            if not tagged_sentence[index-2][0].startswith(\"ask\") and not tagged_sentence[index-2][0].startswith(\"tell\") and not tagged_sentence[index-2][0] == \"told\": \n",
    "                if not tagged_sentence[index+1][1].startswith(\"R\") and not tagged_sentence[index+1][1].startswith(\"V\") and not tagged_sentence[index+1][1].startswith(\"MD\"):\n",
    "                    features_dict[\"whreobj_032\"] += 1 \n",
    "                \n",
    "        if word_tuple in WHO: # WHO = [\"what\", \"where\", \"when\", \"how\", \"whether\", \"why\", \"whoever\", \"whomever\", \"whichever\", \"whenever\", \"whatever\", \"however\"]\n",
    "            #13. direct WH-questions CL-P/Tif + WHO + AUX (where AUX is not part of a contracted form)\n",
    "            if tagged_sentence[index-1][1] == \"X\":  \n",
    "                if tagged_sentence[index+1][1] == \"MD\":\n",
    "                    features_dict[\"whquest_013\"] += 1 \n",
    "            elif tagged_sentence[index+1][1].startswith(\"V\"):\n",
    "                if tagged_sentence[index+1][0] in belist or tagged_sentence[index+1][0] in havelist or tagged_sentence[index+1][0] in dolist:\n",
    "                    features_dict[\"whquest_013\"] += 1\n",
    "       \n",
    "\n",
    "\n",
    "def analyze_there(index, tagged_sentence, features_dict):\n",
    "    if tagged_sentence[index][1] == \"EX\":\n",
    "        features_dict[\"exthere_020\"] += 1 \n",
    "        \n",
    "    \n",
    "def analyze_particle(index, tagged_sentence, features_dict):\n",
    "    word_tuple = tagged_sentence[index]\n",
    "    if index == 0 and word_tuple[0] in discpart:\n",
    "        features_dict[\"discpart_050\"] += 1\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output functions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tagger(tagged_sentence, features_dict):\n",
    "    for index in range(3, len(tagged_sentence)-3): #based on POS, apply different function\n",
    "        current_tag = tagged_sentence[index][1]\n",
    "        if current_tag.startswith(\"V\"):\n",
    "            analyze_verb(index, tagged_sentence, features_dict)\n",
    "        elif current_tag == \"MD\":\n",
    "            analyze_modal(index, tagged_sentence, features_dict)\n",
    "        elif current_tag.startswith(\"N\"):\n",
    "            analyze_noun(index, tagged_sentence, features_dict)\n",
    "        elif current_tag.startswith(\"RB\"):\n",
    "            analyze_adverb(index, tagged_sentence,features_dict)\n",
    "        elif current_tag.startswith(\"J\"):\n",
    "            analyze_adjective(index, tagged_sentence, features_dict)\n",
    "        elif current_tag.startswith(\"I\"):\n",
    "            analyze_preposition(index, tagged_sentence, features_dict)\n",
    "        elif current_tag.startswith(\"W\"):\n",
    "            analyze_wh_word(index, tagged_sentence, features_dict)\n",
    "        elif current_tag.startswith(\"CC\"):\n",
    "            analyze_conjunction(index, tagged_sentence, features_dict)\n",
    "        elif current_tag.startswith(\"RP\"):\n",
    "            analyze_particle(index, tagged_sentence, features_dict)\n",
    "        elif current_tag.startswith(\"DT\"):\n",
    "            analyze_determiner(index, tagged_sentence, features_dict)\n",
    "        elif current_tag.startswith(\"PR\"):\n",
    "            analyze_pronoun(index, tagged_sentence, features_dict)\n",
    "        elif current_tag.startswith(\"EX\"):\n",
    "            analyze_there(index, tagged_sentence, features_dict)\n",
    "    # at some point the order of these elif-statements could be updated using freq counts from our data\n",
    "\n",
    "def MDA_analyzer(filepath):\n",
    "    preprocessed_file = open_reddit_json(filepath) #reads in file, separates into sentences, initializes feature dict\n",
    "    analyze_sentence(preprocessed_file) #updates raw-sentence based counts (i.e. punctuation marks, length)\n",
    "    all_ft_dicts = []\n",
    "\n",
    "    for id in preprocessed_file: #loops through all individual sentences in the file one by one\n",
    "        sentence_dict = preprocessed_file.get(id) #retrieves entire dictionary and all sub-dicts for the given sentence\n",
    "        sentence = sentence_dict[\"body\"] #retrieves sentence only (str)) \n",
    "        features_dict = sentence_dict[\"features\"] #retrieves s for the given sentence\n",
    "        tagged_sentence = tag_sentence(sentence) #tags sentence, returning list of tuples with (word, pos)\n",
    "        POS_tagger(tagged_sentence, features_dict)\n",
    "        all_ft_dicts.append(sentence_dict)\n",
    "    \n",
    "    r = open(os.path.join(dirname, 'results', preprocessed_file, \"_r\"), \"w\")\n",
    "    r.write(all_ft_dicts + '\\n')\n",
    "    r.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tester(practice_sentences, feature):\n",
    "    for practice_sentence in practice_sentences:\n",
    "        tagged_sentence = tag_sentence(practice_sentence)\n",
    "        features_dict = s.copy()\n",
    "        POS_tagger(tagged_sentence, features_dict)\n",
    "        \n",
    "        #put breakpoint on line below\n",
    "        print(practice_sentence, \"//count = \", features_dict[feature]) \n",
    "\n",
    "practice_sentences = [\"He then consequently ate five donuts in a row.\", \n",
    "\"Go buy donuts now else there won't be any left.\", \"I want something else.\", \n",
    "\"Go eat a donut instead of complaining.\", \"I would much rather have a donut now than later.\"]\n",
    "\n",
    "#tester(practice_sentences, \"conjuncts_045\")\n",
    "\n",
    "\n",
    "def process_sent(sent, feat):\n",
    "    tagged_sentence = tag_sentence(sent)\n",
    "    features_dict = s.copy()\n",
    "    POS_tagger(tagged_sentence, features_dict)\n",
    "    return features_dict[feat]\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "#     ram_present = psutil.virtual_memory()[0] >> 30\n",
    "#     if ram_present < 7:\n",
    "#         print(\"WARNING: This is RAM-intensive operation. It cannot continue if you don't have at least 8 GB of RAM.\\nExiting...\")\n",
    "#         sys.exit(0)\n",
    "    \n",
    "#     p = Pool()\n",
    "    \n",
    "#     results = p.map(MDA_analyzer, all_files)\n",
    "#     print(results)\n",
    "    \n",
    "#     p.close()\n",
    "#     p.join()\n",
    "\n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#     executor.map(MDA_analyzer, all_files)\n",
    "\n",
    "# for file in all_files:\n",
    "#     MDA_analyzer(file)\n",
    "\n",
    "print(timedelta(seconds=time.time() - start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "915ec26a27f31d7a8db4bf03f6f41f87084598d7b314c76fa930970a9cbd09a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
