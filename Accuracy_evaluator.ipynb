{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-18 12:55:52,467 loading file C:/Users/gusta/Documents/GitHub/Reddit_MDA/RedditTaggerFinal150.pt\n",
      "2022-07-18 12:56:21,023 SequenceTagger predicts: Dictionary with 49 tags: <unk>, O, LS, NN, VBG, VBZ, DT, RB, JJ, WDT, VBN, IN, CC, ., NNS, VBP, ,, TO, -LRB-, NNP, -RRB-, RBR, JJR, PRP, CD, VB, MD, PRP$, POS, RBS, :, JJS, NNPS, VBD, RP, WRB, WP, SYM, EX, FW, UH, -LSB-, -RSB-, ``, '', PDT, WP$, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "### import and loads the taggers to evaluate\n",
    "from turtle import pos\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "from sklearn import metrics\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "tagger_FLAIR = SequenceTagger.load(\"C:/Users/gusta/Documents/GitHub/Reddit_MDA/RedditTaggerFinal150.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the functions and creates empty list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    '''Takes a sentence and returns it in all lowercase, with punctuation removed, and emojis removed.'''\n",
    "    sentence = str(sentence).strip(string.punctuation).lower()\n",
    "    for emoticon in [\":-)\", \":)\", \";-)\", \":-P\", \";-P\", \":-p\", \";-p\", \":-(\", \";-(\", \":-O\", \"^^\", \"-.-\", \":-$\", \":-\\\\\", \":-/\", \":-|\", \";-/\", \";-\\\\\",\n",
    "                        \":-[\", \":-]\", \":-ยง\", \"owo\", \"*.*\", \";)\", \":P\", \":p\", \";P\", \";p\", \":(\", \";(\", \":O\", \":o\", \":|\", \";/\", \";\\\\\", \":[\", \":]\", \":ยง\"]:\n",
    "        sentence = sentence.replace(emoticon, \"\")\n",
    "    ## emoticons already counted (but not removed) in the analyse_sentence function\n",
    "    ## emojis already counted (but not removed) in the analyse_sentence function\n",
    "    ## links and URLs counted AND removed in the analyse_sentence function\n",
    "    return sentence  \n",
    "\n",
    "def tag_sentence(sentence):\n",
    "    '''Takes a sentence, cleans it with clean_sentence, and tags it using the FLAIR POS tagger. \n",
    "    Adds a look ahead/behind buffer of three items of type (\"X\", \"X\") to prevent negative indices and IndexErrors\n",
    "    Returns a list of tuples of (word, pos_tag).'''\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "    flair_sentence = Sentence(cleaned_sentence)\n",
    "    tagger_FLAIR.predict(flair_sentence)\n",
    "    token_list = []\n",
    "    for label in flair_sentence.get_labels('pos'):\n",
    "        if not label.value in [\"''\", \"``\"]:    \n",
    "            token_list.append(tuple([label.data_point.text] + [label.value])) \n",
    "    empty_look = [(\"X\", \"X\"), (\"X\", \"X\"), (\"X\", \"X\")]\n",
    "    tagged_sentence = empty_look + token_list + empty_look \n",
    "    return tagged_sentence\n",
    "\n",
    "def sentence_tags(w_pos_list):\n",
    "    gold = [x[1] for x in w_pos_list]\n",
    "    words = [x[0] for x in w_pos_list]\n",
    "    nltk_tags = [x[1] for x in pos_tag(words)]\n",
    "    sent = \" \".join(words)\n",
    "    flair_tags = [x[1] for x in tag_sentence(sent)][3:-3]\n",
    "    if len(gold) == len(nltk_tags) == len(flair_tags):\n",
    "        return(gold, nltk_tags, flair_tags)\n",
    "    else:\n",
    "        print(\"Unequal token numbers\")\n",
    "        print(gold)\n",
    "        print(flair_tags)\n",
    "        print(nltk_tags)\n",
    "        return([],[],[],[])\n",
    "\n",
    "gold = []\n",
    "nltk_tags = []\n",
    "flair_tags = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates f-scores with SKLEARN metrics \n",
    "\n",
    "I used these links as reference \n",
    "\n",
    "https://natemccoy.github.io/2016/10/27/evaluatingnltktaggerstutorial.html\n",
    "\n",
    "https://stackoverflow.com/questions/46713629/evaluating-pos-tagger-in-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"C:/Users/gusta/Documents/GitHub/Reddit_MDA/Tagged_JSONS/RC_2005-12_tagged_manual_Batch1_done.txt\") as f:\n",
    "#     for line in f:\n",
    "#         if line.split(\"\\t\")[2] == \"[]\\n\":\n",
    "#             pass\n",
    "#         else:\n",
    "#             print(line.split(\"\\t\")[0])\n",
    "#             sent_raw = line.split(\"\\t\")[2].strip(\"\\n\")\n",
    "#             sent_split = sent_raw.strip(\"[]\").replace(\"'\", \"\").split(\"], [\")\n",
    "#             sentence = [x.split(\", \") for x in sent_split]\n",
    "#             gold += sentence_tags(sentence)[0]\n",
    "#             nltk_tags += sentence_tags(sentence)[1]\n",
    "#             flair_tags += sentence_tags(sentence)[2]\n",
    "\n",
    "with open(\"C:/Users/gusta/Documents/GitHub/Reddit_MDA/Tagged_JSONS/RC_2005-12_tagged_manual_Batch2_done.txt\") as g:\n",
    "    for line in g:\n",
    "        if line.split(\"\\t\")[2] == \"[]\\n\":\n",
    "            pass\n",
    "        else:\n",
    "            print(line.split(\"\\t\")[0])\n",
    "            sent_raw = line.split(\"\\t\")[2].strip(\"\\n\")\n",
    "            sent_split = sent_raw.strip(\"[]\").replace(\"'\", \"\").split(\"], [\")\n",
    "            sentence = [x.split(\", \") for x in sent_split]\n",
    "            gold += sentence_tags(sentence)[0]\n",
    "            nltk_tags += sentence_tags(sentence)[1]\n",
    "            flair_tags += sentence_tags(sentence)[2]\n",
    "\n",
    "# with open(\"C:/Users/gusta/Documents/GitHub/Reddit_MDA/Tagged_JSONS/RC_2005-12_tagged_manual_Batch3_done.txt\") as h:\n",
    "#     for line in h:\n",
    "#         if line.split(\"\\t\")[2] == \"[]\\n\":\n",
    "#             pass\n",
    "#         else:\n",
    "#             print(line.split(\"\\t\")[0])\n",
    "#             sent_raw = line.split(\"\\t\")[2].strip(\"\\n\")\n",
    "#             sent_split = sent_raw.strip(\"[]\").replace(\"'\", \"\").split(\"], [\")\n",
    "#             sentence = [x.split(\", \") for x in sent_split]\n",
    "#             gold += sentence_tags(sentence)[0]\n",
    "#             nltk_tags += sentence_tags(sentence)[1]\n",
    "#             flair_tags += sentence_tags(sentence)[2]\n",
    "\n",
    "# with open(\"C:/Users/gusta/Documents/GitHub/Reddit_MDA/Tagged_JSONS/RC_2005-12_tagged_manual_Batch5_done.txt\") as i:\n",
    "#     for line in i:\n",
    "#         if line.split(\"\\t\")[2] == \"[]\\n\":\n",
    "#             pass\n",
    "#         else:\n",
    "#             print(line.split(\"\\t\")[0])\n",
    "#             sent_raw = line.split(\"\\t\")[2].strip(\"\\n\")\n",
    "#             sent_split = sent_raw.strip(\"[]\").replace(\"'\", \"\").split(\"], [\")\n",
    "#             sentence = [x.split(\", \") for x in sent_split]\n",
    "#             gold += sentence_tags(sentence)[0]\n",
    "#             nltk_tags += sentence_tags(sentence)[1]\n",
    "#             flair_tags += sentence_tags(sentence)[2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk_classification = metrics.classification_report(gold, nltk_tags,)\n",
    "flair_classification = metrics.classification_report(gold, flair_tags,)\n",
    "\n",
    "with open(\"Tagging_accuracy_reports.txt\", \"w\") as p:\n",
    "    p.write(nltk_classification)\n",
    "    p.write(\"\\n\\n\\n\")\n",
    "    p.write(flair_classification)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "915ec26a27f31d7a8db4bf03f6f41f87084598d7b314c76fa930970a9cbd09a8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
